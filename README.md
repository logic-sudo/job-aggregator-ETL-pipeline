# Job Aggregator ETL Pipeline (Databricks + PySpark)

This project is an end-to-end **ETL pipeline** built on **Databricks** that collects job postings from Google Jobs (via SerpAPI), processes them using **PySpark + Delta Lake**, and generates insights such as job demand by company, city, and role.

The pipeline follows the **Medallion architecture (Bronze ‚Üí Silver ‚Üí Gold)** and includes visualizations for analytics.

---

## üöÄ Tech Stack

- **Databricks**
- **Apache Spark / PySpark**
- **Delta Lake**
- **Python**
- **SerpAPI (Google Jobs API)**
- **Pandas**
- **Matplotlib**

---

## üß± Architecture Overview

High-level flow of the pipeline:

1. **Bronze Layer** ‚Äì Ingest raw job postings from SerpAPI  
2. **Silver Layer** ‚Äì Clean & transform data (trim, deduplicate, select useful fields)  
3. **Gold Layer** ‚Äì Aggregate and prepare data for analytics and visualizations

‚öôÔ∏è Setup & Running (Local)

Although this project was built in Databricks, it can be adapted locally.

1Ô∏è‚É£ Install dependencies
pip install -r requirements.txt

2Ô∏è‚É£ Configure API Key

Set your SerpAPI API key as an environment variable:

export API_KEY="your-serpapi-key-here"

3Ô∏è‚É£ Run ETL Scripts (if using modular src/ files)
# Bronze ingestion
spark-submit src/bronze_layer.py

# Silver transforms
spark-submit src/silver_layer.py

# Gold aggregations & visualizations
spark-submit src/gold_layer.py
